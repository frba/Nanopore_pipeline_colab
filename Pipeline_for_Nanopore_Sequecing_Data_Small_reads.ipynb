{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frba/Nanopore_pipeline_colab/blob/main/Pipeline_for_Nanopore_Sequecing_Data_Small_reads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline for Nanopore Sequencing Data"
      ],
      "metadata": {
        "id": "_xYmQkFzZ5Ve"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfsx7bY4XUSS"
      },
      "source": [
        "## Step 1 - Upload files and Installing packages\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   Upload your fastq files containing reads (raw data from plasmidsaurus)\n",
        "2.   Upload the minibar barcode_file.txt - [example_barcode_file](https://liveconcordia.sharepoint.com/:t:/t/GenomeFoundryTeam/EQsYiSKVWo5Oli4UnBnkIEABReHpAAEuzrEk2Cypi6QlMw?e=1tqfym)\n",
        "3.   Upload fasta file containing the expected sequences - [example_fasta_template](https://liveconcordia.sharepoint.com/:u:/t/GenomeFoundryTeam/Eb9_w8lNcoFEmh9iUuME76EBpCtpj6ZHnsJcx9UtqFyuOQ?e=4SjzwQ)\n",
        "4.   Upload a [Echo transfer.csv](https://liveconcordia.sharepoint.com/:x:/t/GenomeFoundryTeam/EYVzHEZPGb9DqxJDN56S_6kBc1_-whiPIm1jkmkiuh-U3A?e=tD3xl6) file (input file used in Echo)\n",
        "\n",
        "```markdown\n",
        "5.   **Run the Step 1, it will crash and restart the runtime. Run it again and the following cells.**\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MHiIgbQY4h5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7361e7ab-1748-4e26-ef5b-78f5a78e671f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'bioconda' already in 'channels' list, moving to the top\n",
            "Channels:\n",
            " - conda-forge\n",
            " - bioconda\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Channels:\n",
            " - bioconda\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Channels:\n",
            " - bioconda\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Channels:\n",
            " - bioconda\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/site-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from biopython) (2.1.3)\n",
            "Requirement already satisfied: edlib in /usr/local/lib/python3.10/site-packages (1.3.9.post1)\n"
          ]
        }
      ],
      "source": [
        "version = !conda --version\n",
        "if version[0] == \"/bin/bash: line 1: conda: command not found\":\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "!conda config --add channels bioconda\n",
        "!conda install -c conda-forge -c bioconda mmseqs2\n",
        "!conda install -c bioconda blast\n",
        "!conda install flye\n",
        "!conda install -c bioconda seqkit\n",
        "%pip install biopython\n",
        "%pip install edlib\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "import subprocess\n",
        "import shutil\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS6Esjkrq6UW"
      },
      "source": [
        "## Step 1.1 - Installing Tools: SPOA, Filtlong, Minibar\n",
        "\n",
        "1.   Installing SPOA (De novo assembler tool),\n",
        "2.   Filtlong (Filtering tool), and  \n",
        "3.   Minibar (Demultiplex tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Skep5V6nX_a-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dac3db9-b194-4274-ea9d-e98d75fd5b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spoa'...\n",
            "remote: Enumerating objects: 1360, done.\u001b[K\n",
            "remote: Counting objects: 100% (262/262), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 1360 (delta 213), reused 197 (delta 187), pack-reused 1098 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1360/1360), 526.51 KiB | 16.45 MiB/s, done.\n",
            "Resolving deltas: 100% (905/905), done.\n",
            "/content/spoa\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/lib/python3.10/dist-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:1953 (message):\n",
            "  Calling FetchContent_Populate(bioparser) is deprecated, call\n",
            "  FetchContent_MakeAvailable(bioparser) instead.  Policy CMP0169 can be set\n",
            "  to OLD to allow FetchContent_Populate(bioparser) to be called directly for\n",
            "  now, but the ability to call it with declared details will be removed\n",
            "  completely in a future version.\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:130 (FetchContent_Populate)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found ZLIB: /usr/local/lib/libz.so (found suitable version \"1.2.13\", minimum required is \"1.2.8\")\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/lib/python3.10/dist-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:1953 (message):\n",
            "  Calling FetchContent_Populate(biosoup) is deprecated, call\n",
            "  FetchContent_MakeAvailable(biosoup) instead.  Policy CMP0169 can be set to\n",
            "  OLD to allow FetchContent_Populate(biosoup) to be called directly for now,\n",
            "  but the ability to call it with declared details will be removed completely\n",
            "  in a future version.\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:146 (FetchContent_Populate)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/lib/python3.10/dist-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:1953 (message):\n",
            "  Calling FetchContent_Populate(googletest) is deprecated, call\n",
            "  FetchContent_MakeAvailable(googletest) instead.  Policy CMP0169 can be set\n",
            "  to OLD to allow FetchContent_Populate(googletest) to be called directly for\n",
            "  now, but the ability to call it with declared details will be removed\n",
            "  completely in a future version.\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:165 (FetchContent_Populate)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at build/_deps/googletest-src/CMakeLists.txt:4 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "\u001b[0mCMake Deprecation Warning at build/_deps/googletest-src/googlemock/CMakeLists.txt:45 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at build/_deps/googletest-src/googletest/CMakeLists.txt:56 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at build/_deps/googletest-src/googletest/cmake/internal_utils.cmake:243 (find_package):\n",
            "  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules\n",
            "  are removed.  Run \"cmake --help-policy CMP0148\" for policy details.  Use\n",
            "  the cmake_policy command to set the policy and suppress this warning.\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  build/_deps/googletest-src/googletest/CMakeLists.txt:91 (include)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found PythonInterp: /usr/local/bin/python (found version \"3.10.13\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Configuring done (6.4s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/spoa/build\n",
            "make: Entering directory '/content/spoa/build'\n",
            "make[1]: Entering directory '/content/spoa/build'\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/spoa.dir/src/alignment_engine.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/spoa.dir/src/graph.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/spoa.dir/src/simd_alignment_engine_dispatcher.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/spoa.dir/src/sisd_alignment_engine.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/spoa.dir/src/version.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX static library lib/libspoa.a\u001b[0m\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "[ 42%] Built target spoa\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/spoa_exe.dir/src/main.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable bin/spoa\u001b[0m\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "[ 57%] Built target spoa_exe\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "[ 64%] \u001b[32mBuilding CXX object _deps/googletest-build/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libgtest.a\u001b[0m\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "[ 71%] Built target gtest\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "[ 78%] \u001b[32mBuilding CXX object _deps/googletest-build/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libgtest_main.a\u001b[0m\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "[ 85%] Built target gtest_main\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "make[2]: Entering directory '/content/spoa/build'\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/spoa_test.dir/test/spoa_test.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable bin/spoa_test\u001b[0m\n",
            "make[2]: Leaving directory '/content/spoa/build'\n",
            "[100%] Built target spoa_test\n",
            "make[1]: Leaving directory '/content/spoa/build'\n",
            "make: Leaving directory '/content/spoa/build'\n",
            "/content\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 45339  100 45339    0     0   306k      0 --:--:-- --:--:-- --:--:--  307k\n",
            "Cloning into 'Filtlong'...\n",
            "remote: Enumerating objects: 427, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 427 (delta 15), reused 23 (delta 9), pack-reused 392 (from 1)\u001b[K\n",
            "Receiving objects: 100% (427/427), 5.08 MiB | 30.96 MiB/s, done.\n",
            "Resolving deltas: 100% (257/257), done.\n",
            "/content/Filtlong\n",
            "g++ -std=c++11 -O3 -Wall -Wextra -pedantic -mtune=native -c -o src/arguments.o src/arguments.cpp\n",
            "g++ -std=c++11 -O3 -Wall -Wextra -pedantic -mtune=native -c -o src/main.o src/main.cpp\n",
            "g++ -std=c++11 -O3 -Wall -Wextra -pedantic -mtune=native -c -o src/misc.o src/misc.cpp\n",
            "g++ -std=c++11 -O3 -Wall -Wextra -pedantic -mtune=native -c -o src/kmers.o src/kmers.cpp\n",
            "g++ -std=c++11 -O3 -Wall -Wextra -pedantic -mtune=native -c -o src/read.o src/read.cpp\n",
            "\u001b[01m\u001b[Ksrc/arguments.cpp:\u001b[m\u001b[K In constructor ‘\u001b[01m\u001b[KArguments::Arguments(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/arguments.cpp:155:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcatching polymorphic type ‘\u001b[01m\u001b[Kclass args::Help\u001b[m\u001b[K’ by value [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcatch-value=\u0007-Wcatch-value=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  155 |     catch (args::\u001b[01;35m\u001b[KHelp\u001b[m\u001b[K) {\n",
            "      |                  \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/arguments.cpp:160:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcatching polymorphic type ‘\u001b[01m\u001b[Kclass args::ParseError\u001b[m\u001b[K’ by value [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcatch-value=\u0007-Wcatch-value=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  160 |     catch (args::ParseError \u001b[01;35m\u001b[Ke\u001b[m\u001b[K) {\n",
            "      |                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/arguments.cpp:165:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcatching polymorphic type ‘\u001b[01m\u001b[Kclass args::ValidationError\u001b[m\u001b[K’ by value [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcatch-value=\u0007-Wcatch-value=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  165 |     catch (args::ValidationError \u001b[01;35m\u001b[Ke\u001b[m\u001b[K) {\n",
            "      |                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "g++ -std=c++11 -O3 -Wall -Wextra -pedantic -mtune=native -o bin/filtlong src/arguments.o src/main.o src/misc.o src/kmers.o src/read.o -lz\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rvaser/spoa\n",
        "%cd ./spoa\n",
        "!cmake -B build -DCMAKE_BUILD_TYPE=Release\n",
        "!make -C build\n",
        "%cd ..\n",
        "\n",
        "!curl https://raw.githubusercontent.com/calacademy-research/minibar/master/minibar.py > minibar.py\n",
        "!chmod +x minibar.py\n",
        "\n",
        "!git clone https://github.com/rrwick/Filtlong.git\n",
        "%cd ./Filtlong\n",
        "!make -j\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlhkY3tzrUR0"
      },
      "source": [
        "##Step 2 - Set variable names (**Needs input from User**)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "From the uploaded files, set the filename to each variable below:\n",
        "\n",
        "*   barcodes_file  (a .txt file with the sequence of pair of barcodes).\n",
        "*   template_file (a fasta file with the expected sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdQnxARZkzv4"
      },
      "outputs": [],
      "source": [
        "# prompt: combine multiple fastq files in one unique file\n",
        "# Assuming ONLY your raw fastq files are in the current directory\n",
        "\n",
        "!cat *.fastq >> merged_fastq_file.fastq # merge all raw data in one unique fastq file\n",
        "basecalled_file = 'merged_fastq_file.fastq' # do not modify this filename\n",
        "\n",
        "barcode_file = 'barcodes.txt' #<--- Rename input file\n",
        "template_file = 'genes.fasta' #<--- Rename input file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMCm_m8rDDF"
      },
      "source": [
        "##Step 3 - Filtering with Filtlong (Needs input from User)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Set **min** and **max** length of the reads\n",
        "\n",
        "_Filtlong will remove reads too small or too long from the raw data and return a file called filtered.fastq.gz_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcVIXgHyE2rl"
      },
      "outputs": [],
      "source": [
        "!Filtlong/bin/filtlong --min_length 400 --max_length 800 {basecalled_file} | gzip > filtered.fastq.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wPvYLZn3gGA"
      },
      "source": [
        "##Step 4 - Demultiplexing with Minibar\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "_Minibar will use the barcode.txt file + filtered.fastq.gs to demux the reads in bins listed in barcode.txt file_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfdmJIsspzhP"
      },
      "outputs": [],
      "source": [
        "!./minibar.py -e 4 -F -T -M 1 -P demux_minibar_1_ {barcode_file} filtered.fastq.gz\n",
        "\n",
        "!zip -R 'demultiplexed.zip' 'demux_minibar*' -qq\n",
        "!zip -R 'demultiplexed.zip' {barcode_file} -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhXprMgQNd1M"
      },
      "source": [
        "##Step 5 - Stats information about the demultiplexed reads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5.1 Count reads per barcode\n",
        "_Returns a file called count_barcodes.csv with a list of barcodes and number of reads in each._"
      ],
      "metadata": {
        "id": "ihXYCYM7kdIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BcWZ9BcFOq7"
      },
      "outputs": [],
      "source": [
        "index = pd.read_csv(barcode_file, delimiter='\\t', index_col=0).index\n",
        "headers = [1,2,3,4]\n",
        "data = dict()\n",
        "for run in headers[:]:\n",
        "    data[run] = dict()\n",
        "    prefix = f'demux_minibar_{run}_'\n",
        "    file_list = glob.glob(f'{prefix}*')\n",
        "    for file in file_list:\n",
        "        sample=Path(file).stem[file.find(prefix)+len(prefix):]\n",
        "        count = len(list(SeqIO.parse(file, format='fastq')))\n",
        "        data[run][sample] = count\n",
        "\n",
        "pd.DataFrame(data).to_csv('count_barcodes.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.2 Quality of reads by bin (Optional)\n",
        "_Returns a file called quality_barcodes.csv with a list of barcodes and number of reads, and quality information (Q20%, Q30%, average quality) and more._\n",
        "\n",
        "filename            | format | type |... | Q20(%)|... |\n",
        "--------------------|--------|------|----|-------|----|\n",
        "demux_FP1-RP1.fastaq| FASTQ  | DNA  |... | 88.21 |... |\n",
        "demux_FP1-RP2.fastaq| FASTQ  | DNA  |... | 96.79 |... |\n",
        "demux_FP1-RP3.fastaq| FASTQ  | DNA  |... | 79.44 |... |"
      ],
      "metadata": {
        "id": "UYn50ASuTBQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_and_merge_quality():\n",
        "    \"\"\"Calculates quality of fastq files and merges the output into quality_barcodes.csv.\"\"\"\n",
        "    seqkit_cmd = f\"seqkit stats demux_minibar_*.fastq -a\"\n",
        "    result = subprocess.run(seqkit_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # Split the output into lines and process each line\n",
        "    lines = result.stdout.strip().split('\\n')\n",
        "\n",
        "    # Extract header and data\n",
        "    header = lines[0].split()\n",
        "    data = [line.split() for line in lines[1:]]\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data, columns=header)\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    df.to_csv('quality_barcodes.csv', index=False)\n",
        "\n",
        "calculate_and_merge_quality()"
      ],
      "metadata": {
        "id": "QQUyBOLqTFwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3 Quality of each read in a demux barcode pair (Optional)\n",
        "_Returns a file called quality_reads.csv with a list of all reads in each barcode and average quality information._\n",
        "\n",
        "filename            | seqid   | qual\n",
        "--------------------|---------|---------\n",
        "demux_FP1-RP1.fastaq| 974_ABC | 25.89\n",
        "demux_FP1-RP2.fastaq| 128_CDB | 30.01\n",
        "demux_FP1-RP3.fastaq| 579_CDB | 18.10"
      ],
      "metadata": {
        "id": "aFxk3YqxoAlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "def run(fastq_file, references, i, j):\n",
        "    seqkit_cmd = f\"seqkit fx2tab {fastq_file} -n -q\"\n",
        "\n",
        "    # Run the seqkit command and capture the output\n",
        "    result = subprocess.run(seqkit_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # Convert output to DataFrame and add filename column\n",
        "    df = pd.read_csv(io.StringIO(result.stdout), sep='\\t', header=None, names=['seq', 'qual'])\n",
        "    df.insert(0, 'file', fastq_file)  # Insert filename in the first column\n",
        "\n",
        "    # Append DataFrame to quality_reads.csv\n",
        "    if not os.path.isfile('quality_reads.csv'):\n",
        "        df.to_csv('quality_reads.csv', index=False, mode='w')\n",
        "    else:\n",
        "        df.to_csv('quality_reads.csv', index=False, header=False, mode='a')\n",
        "\n",
        "\n",
        "for i in range(1, 10):  # Changed to range(1, 2) to iterate once\n",
        "  for j in range(1, 10):  # Changed to range(1, 2) to iterate once\n",
        "    reads_path = f\"demux_minibar_1_FP{i}-RP{j}.fastq\"\n",
        "    if os.path.exists(reads_path):\n",
        "        run(reads_path, template_file, i, j)"
      ],
      "metadata": {
        "id": "f02jsn5noASB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGFPfJmDwi0Z"
      },
      "source": [
        "##Step 6 - De Novo Consensus with SPOA\n",
        "\n",
        "---\n",
        "_SPOA will create a de novo consensus sequence for each pair of barcode._\n",
        "**bold text**\n",
        "_A zipfile: assembled.zip will have all consensus created._\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SypYO6hhHYeA"
      },
      "outputs": [],
      "source": [
        "files = glob.glob('demux_minibar_*.fastq')\n",
        "for file in files:\n",
        "    cmd = f'./spoa/build/bin/spoa {file} > {file.replace(\"fastq\", \"fasta\")}'\n",
        "    !{cmd}\n",
        "\n",
        "#Compress output files in assembled.zip\n",
        "!zip -R 'assembled.zip' 'demux_*.fasta' -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNoyOKL7Z9Nc"
      },
      "source": [
        "##Step 7 - Polishing consensus with Flye (Needs User input)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Variable **estimated_size** needs to be updated!!\n",
        "\n",
        "\n",
        "_Fasta files with consensus will be polished by Flye and return a file *_polished.fasta and a zipfile polished.zip_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i32FUTQmJ5dU"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 21):\n",
        "  for j in range(1, 21):\n",
        "    reads_path = f\"demux_minibar_1_FP{i}-RP{j}.fastq\" #Filtered and demultiplexed reads\n",
        "    write_path = f\"flye_output_FP{i}-RP{j}\" #Folder name\n",
        "    polish_target = f\"demux_minibar_1_FP{i}-RP{j}.fasta\" #Output file from SPOA\n",
        "    estimated_size = 600 #<---- PLEASE, CHECK THE ESTIMATED LENGTH!\n",
        "\n",
        "    if os.path.exists(reads_path):\n",
        "      flye_command = f\"flye --nano-hq {reads_path} --out-dir {write_path} --genome-size {estimated_size} --meta --keep-haplotypes --polish-target {polish_target}\"\n",
        "\n",
        "      try:\n",
        "          # Run the flye command, capture stderr, and check for errors\n",
        "          result = subprocess.run(flye_command, shell=True, check=True, capture_output=True, text=True)\n",
        "          print(result.stdout)  # Print standard output if successful\n",
        "      except subprocess.CalledProcessError as e:\n",
        "          print(f\"Error running Flye: {e}\")\n",
        "          print(f\"Flye stderr output:\\n{e.stderr}\")  # Print standard error for debugging\n",
        "\n",
        "      #Move the polished file once it has been created\n",
        "      !mv \"{write_path}/polished_1.fasta\" \"{polish_target.replace(\".fasta\", \"_polished.fasta\")}\"\n",
        "      print(f\"Polished file: {polish_target}\")\n",
        "\n",
        "# Delete all folders starting with \"flye_output\" and their contents\n",
        "for folder_name in glob.glob(\"flye_output*\"):\n",
        "    shutil.rmtree(folder_name)\n",
        "\n",
        "# Delete files ending with *.fai\n",
        "for file in glob.glob(\"*.fai\"):\n",
        "    os.remove(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om3pONvQDsVV"
      },
      "source": [
        "##Step 8 - Rename seqid with filename\n",
        "\n",
        "_After running SPOA the seqid in the fasta files is renamed to: 'consensus'._\n",
        "\n",
        "_To keep the trackability of the reads, it needs to be renamed with the filename._\n",
        "\n",
        "_This step will rename all *_polished.fasta's seqid with their filename._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CltiVLAoJ5eq"
      },
      "outputs": [],
      "source": [
        "files = glob.glob('*_polished.fasta')\n",
        "\n",
        "for file in files:\n",
        "    name = file.replace('.fasta', '')\n",
        "    record = SeqIO.read(file, 'fasta')\n",
        "    record.id = name\n",
        "    record.name = name\n",
        "    record.description = 'filtered with Filtlong, assembled with SPOA and polish with Flye'\n",
        "    SeqIO.write(record, file, 'fasta')\n",
        "\n",
        "# Create a zip file named 'polished.zip'\n",
        "with zipfile.ZipFile('polished.zip', 'w') as zipf:\n",
        "    # Find all files ending with '_polished.fasta'\n",
        "    for file in glob.glob(\"*_polished.fasta\"):\n",
        "        # Add each file to the zip archive\n",
        "        zipf.write(file)\n",
        "        print(f\"Added to zip: {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 9 - Alignment Tools"
      ],
      "metadata": {
        "id": "41-23yGYrWIT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmZ41pLXEAec"
      },
      "source": [
        "###Step 9.1 Run MMSeqs (Sequence Alignment)\n",
        "\n",
        "_Three files are outputed in this step:_\n",
        "\n",
        "\n",
        "*   AlignResult.tsv (All alignment combinations)\n",
        "*   TopAlignments_bygene (Drop duplicates of templates and keep the highest scores).\n",
        "*   TopAlignments_bybarcode (Drop duplicates of barcodes and keep only the highest alignment scores).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgLvdTzXRTIv"
      },
      "outputs": [],
      "source": [
        "def run_mmseq(queries, references):\n",
        "    for query in queries:\n",
        "        !cat {query} >> mmseq_query.fa # merge all *_polished.fasta in one unique file to use it as input for mmseqs2\n",
        "\n",
        "    # Run mmseqs\n",
        "    mmseq_cmd = f\"mmseqs easy-search mmseq_query.fa {references} AlignResult.tsv tmp --search-type 3\"\n",
        "    !{mmseq_cmd}\n",
        "    all_hits = pd.read_csv('AlignResult.tsv', index_col=None, sep='\\t', header=None)\n",
        "    columns = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore']\n",
        "    all_hits.columns = columns\n",
        "\n",
        "    # Drop duplicates of templates\n",
        "    top_hits = all_hits.sort_values(by='bitscore', ascending=False).drop_duplicates(subset='sseqid')\n",
        "    top_hits.to_csv('TopAlignments_bygene.tsv', sep='\\t', index=False)\n",
        "\n",
        "    # Drop duplicates of barcodes\n",
        "    top_hits = all_hits.sort_values(by='bitscore', ascending=False).drop_duplicates(subset='qseqid')\n",
        "    top_hits.to_csv('TopAlignments_bybarcode.tsv', sep='\\t', index=False)\n",
        "\n",
        "run_mmseq(glob.glob('*_polished.fasta'), template_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMd2VJRtUp6c"
      },
      "outputs": [],
      "source": [
        "pd.read_csv('TopAlignments.tsv', index_col=None, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FTZhPNOQDx7"
      },
      "source": [
        "###Step 9.2 - Run NCBI Blastn\n",
        "\n",
        "_returns a called blast_results.tsv_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJrd9yxaQDNE"
      },
      "outputs": [],
      "source": [
        "def run_blastn(queries, references):\n",
        "\n",
        "    if not os.path.exists(\"mmseq_query.fa\"):\n",
        "      for query in queries:\n",
        "          !cat {query} >> mmseq_query.fa # merge all *_polished.fasta in one unique file to use it as input for blastn\n",
        "\n",
        "    # Create a database file with the templates\n",
        "    blastdb_cmd = f\"makeblastdb -in {template_file} -out blastdb -dbtype nucl\"\n",
        "    !{blastdb_cmd}\n",
        "\n",
        "    # Run BlastN and output a file: blast_results.tsv\n",
        "    blast_cmd = f\"blastn -query mmseq_query.fa -db blastdb -out blast_results.tsv -outfmt '6 std qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore sseq'\"\n",
        "    !{blast_cmd}\n",
        "\n",
        "run_blastn(glob.glob('*_polished.fasta'), template_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10 - Add extra info in the Top Alignment files"
      ],
      "metadata": {
        "id": "rjKCG-KDV0Kd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNsY1fu-bOwl"
      },
      "source": [
        "###Step 10.1 - Add number of reads in TopAlignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCdGXwk9bOI6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def add_read_count(top_alignments_file):\n",
        "    \"\"\"\n",
        "    Adds read count to the TopAlignments file.\n",
        "\n",
        "    Args:\n",
        "        top_alignments_file: Path to the TopAlignments file (tsv).\n",
        "    \"\"\"\n",
        "    # Load the TopAlignments and count_barcodes files\n",
        "    top_alignments = pd.read_csv(top_alignments_file, sep='\\t')\n",
        "    count_barcodes = pd.read_csv('count_barcodes.csv', index_col=0)\n",
        "\n",
        "    # Select the first two columns from count_barcodes\n",
        "    count_barcodes_subset = count_barcodes[['1']]\n",
        "\n",
        "    # Extract the barcode from the TopAlignments 'qseqid' column\n",
        "    top_alignments['barcode'] = top_alignments['qseqid'].str.extract(r'demux_minibar_1_(.*)_polished')\n",
        "\n",
        "    # Merge the two DataFrames based on the barcode\n",
        "    merged_df = pd.merge(top_alignments, count_barcodes_subset, left_on='barcode', right_index=True, how='left')\n",
        "\n",
        "    # Rename the merged column to 'read_count'\n",
        "    merged_df = merged_df.rename(columns={'1': 'read_count'})\n",
        "\n",
        "    # Reorder columns to place the new columns at the beginning\n",
        "    cols = ['barcode', 'read_count'] + [col for col in merged_df.columns if col not in ['barcode', 'read_count']]\n",
        "    merged_df = merged_df[cols]\n",
        "\n",
        "    # Save the updated TopAlignments file\n",
        "    merged_df.to_csv(top_alignments_file, sep='\\t', index=False)\n",
        "\n",
        "# Call the function for both files\n",
        "add_read_count('TopAlignments_bybarcode.tsv')\n",
        "add_read_count('TopAlignments_bygene.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XBkCDMUhhOK"
      },
      "source": [
        "###Step 10.2 - Add Plate Well information in Top Alignments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvDE5kGdhw_B"
      },
      "outputs": [],
      "source": [
        "# prompt: Looking into a .csv file called *echo_transfer\n",
        "# search the barcode (FP1-RP1) split them by -\n",
        "# And search in the csv file that looks like this:\n",
        "# Sample_ID, Sample Name, Source Plate, Source Plate Name, Source Well, Destination Plate Name, Destination Well, Volume,\n",
        "# pY108_FP1_1, pY108_FP_1, GF0001254, MOTY_OL_001, A1, GF0001264, GF0001264, A1, 25\n",
        "# pY108_RP1_1, pY108_RP_1, GF0001254, MOTY_OL_001, C1, GF0001264, GF0001264, A1, 25\n",
        "# pY108_FP1_1, pY108_FP_1, GF0001254, MOTY_OL_001, A1, GF0001264, GF0001264, A2, 25\n",
        "# pY108_RP2_1, pY108_RP_1, GF0001254, MOTY_OL_001, C1, GF0001264, GF0001264, A2, 25\n",
        "# The barcode FP1-RP1 will be separated like this: FP1 and RP1 will share the same Destination Well + Destination Plate Name.\n",
        "# For each barcode in ToAlignments_2.tsv check in the *echo_transfer* the Destination Plate Name and Destination Well and add the information in the TopAlignments_2.tsv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def add_plate_well(top_alignments_file):\n",
        "\n",
        "  # Load the TopAlignments_2.tsv file\n",
        "  top_alignments = pd.read_csv(top_alignments_file, sep='\\t')\n",
        "\n",
        "  # Load the echo_transfer.csv file\n",
        "  echo_transfer = pd.read_csv('409_echo_echo_transfer_draw.csv')\n",
        "\n",
        "  # Function to extract FP and RP barcodes from qseqid\n",
        "  def extract_barcodes(barcode):\n",
        "    fp_barcode = barcode.split('-')[0]\n",
        "    rp_barcode = barcode.split('-')[1]\n",
        "    return fp_barcode, rp_barcode\n",
        "\n",
        "  # Add new columns for Destination Plate Name and Destination Well\n",
        "  top_alignments['Destination Plate Name'] = None\n",
        "  top_alignments['Destination Well'] = None\n",
        "\n",
        "  # Iterate through the TopAlignments_2.tsv file\n",
        "  for index, row in top_alignments.iterrows():\n",
        "    fp_barcode, rp_barcode = extract_barcodes(row['barcode'])\n",
        "\n",
        "    if fp_barcode and rp_barcode:\n",
        "      # Search for matching barcodes in echo_transfer.csv\n",
        "      matching_rows_fp = echo_transfer[echo_transfer['Sample Name'].str.contains(fp_barcode)]\n",
        "      matching_rows_rp = echo_transfer[echo_transfer['Sample Name'].str.contains(rp_barcode)]\n",
        "\n",
        "      # Check if both barcodes have matches\n",
        "      if not matching_rows_fp.empty and not matching_rows_rp.empty:\n",
        "        # Find common Destination Plate Name and Destination Well\n",
        "        common_rows = pd.merge(\n",
        "          matching_rows_fp[['Destination Plate Name', 'Destination Well']],\n",
        "          matching_rows_rp[['Destination Plate Name', 'Destination Well']],\n",
        "          on=['Destination Plate Name', 'Destination Well'],\n",
        "          how='inner'\n",
        "        )\n",
        "\n",
        "        if not common_rows.empty:\n",
        "          # Get the first common row (assuming there's only one)\n",
        "          first_common_row = common_rows.iloc[0]\n",
        "          top_alignments.at[index, 'Destination Plate Name'] = first_common_row['Destination Plate Name']\n",
        "          top_alignments.at[index, 'Destination Well'] = first_common_row['Destination Well']\n",
        "\n",
        "  # Reorder columns to place Destination Plate Name and Destination Well at the beginning\n",
        "  cols = ['Destination Plate Name', 'Destination Well'] + [col for col in top_alignments.columns if col not in ['Destination Plate Name', 'Destination Well']]\n",
        "  top_alignments = top_alignments[cols]\n",
        "\n",
        "  # Save the updated TopAlignments.tsv file\n",
        "  top_alignments.to_csv(top_alignments_file, sep='\\t', index=False)\n",
        "\n",
        "\n",
        "# Call the function for both files\n",
        "add_plate_well('TopAlignments_bybarcode.tsv')\n",
        "add_plate_well('TopAlignments_bygene.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11 - Clean up temp files"
      ],
      "metadata": {
        "id": "MubMDolbWNPh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qwlVmG0nwXc"
      },
      "outputs": [],
      "source": [
        "# Delete all *.fasta files after adding them to the zip\n",
        "for file in glob.glob(\"*.fasta\"):\n",
        "    os.remove(file)\n",
        "    print(f\"Deleted file: {file}\")\n",
        "\n",
        "# Delete all *.fastq files after adding them to the zip\n",
        "for file in glob.glob(\"*.fastq\"):\n",
        "    os.remove(file)\n",
        "    print(f\"Deleted file: {file}\")\n",
        "\n",
        "# Delete all blastdb.* files\n",
        "for file in glob.glob(\"blastdb.*\"):\n",
        "    os.remove(file)\n",
        "    print(f\"Deleted file: {file}\")\n",
        "\n",
        "# Delete all quality* files\n",
        "for file in glob.glob(\"quality*\"):\n",
        "    os.remove(file)\n",
        "    print(f\"Deleted file: {file}\")\n",
        "\n",
        "# Delete all *.stats files\n",
        "for file in glob.glob(\"*.stats\"):\n",
        "    os.remove(file)\n",
        "    print(f\"Deleted file: {file}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}